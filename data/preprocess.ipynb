{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "This notebook includes all the code to preprocess datasets for the experiments in the paper. There will be two main parts: the first half is for row pairs preparation, while the second half is for attribute pairs preparation."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Row Pairs Preparation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training & Validation Set\n",
    "\n",
    "The magellan datasets and wdc dataset need different preparation steps. We will first prepare the magellan datasets."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from autogluon.tabular import TabularPredictor"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Magellan Datasets"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "magellan_dirs = {\n",
    "    'abt': 'raw/abt_buy', 'amgo': 'raw/amazon_google',\n",
    "    'beer': 'raw/beer', 'dbac': 'raw/dblp_acm',\n",
    "    'dbgo': 'raw/dblp_scholar', 'foza': 'raw/fodors_zagat',\n",
    "    'itam': 'raw/itunes_amazon', 'waam': 'raw/walmart_amazon',\n",
    "}\n",
    "\n",
    "magellan_rename_columns = {\n",
    "    'abt': ['id', 'name', 'description', 'price'], 'amgo': ['id', 'name', 'manufacturer', 'price'],\n",
    "    'beer': ['id', 'name', 'factory', 'style', 'ABV'], 'dbac': ['id', 'title', 'authors', 'venue', 'year'],\n",
    "    'dbgo': ['id', 'title', 'authors', 'venue', 'year'], 'foza': ['id', 'name', 'address', 'city', 'phone', 'type', 'class'],\n",
    "    'itam': ['id', 'name', 'artist', 'album', 'genre', 'price', 'copyright', 'time', 'released'],\n",
    "    'waam': ['id', 'name', 'category', 'brand', 'modelno', 'price'],\n",
    "}\n",
    "\n",
    "magellan_drop_columns = {\n",
    "    'abt': ['description'], 'amgo': ['manufacturer'], 'beer': [], 'dbac': [], 'dbgo': [], 'foza': [], 'itam': [],\n",
    "    'waam': ['category', 'brand'],\n",
    "}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def merge_with_id(tableA, tableB, id_pairs):\n",
    "    left_merged = pd.merge(tableA, id_pairs, left_on='id', right_on='ltable_id')\n",
    "    left_right_merged = pd.merge(left_merged, tableB, left_on='rtable_id', right_on='id', suffixes=('_l', '_r'))\n",
    "    left_right_merged.drop(columns=['ltable_id', 'rtable_id', 'id_l', 'id_r'], inplace=True)\n",
    "    return left_right_merged"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def prepare_magellan_row_pairs(dirs: dict, rename_columns: dict, drop_columns: dict):\n",
    "    for d_name in dirs:\n",
    "        tableA = pd.read_csv(os.path.join(dirs[d_name], 'tableA.csv'))\n",
    "        tableB = pd.read_csv(os.path.join(dirs[d_name], 'tableB.csv'))\n",
    "        tableA.columns = rename_columns[d_name]\n",
    "        tableB.columns = rename_columns[d_name]\n",
    "        tableA.drop(columns=drop_columns[d_name], inplace=True)\n",
    "        tableB.drop(columns=drop_columns[d_name], inplace=True)\n",
    "\n",
    "        train_id_pairs = pd.read_csv(os.path.join(dirs[d_name], 'train.csv'))\n",
    "        valid_id_pairs = pd.read_csv(os.path.join(dirs[d_name], 'valid.csv'))\n",
    "        test_id_pairs = pd.read_csv(os.path.join(dirs[d_name], 'test.csv'))\n",
    "        train_df = merge_with_id(tableA, tableB, train_id_pairs)\n",
    "        valid_df = merge_with_id(tableA, tableB, valid_id_pairs)\n",
    "        test_df = merge_with_id(tableA, tableB, test_id_pairs)\n",
    "\n",
    "        if not os.path.exists(f'prepared/{d_name}'):\n",
    "            os.makedirs(f'prepared/{d_name}')\n",
    "        train_df.to_csv(f'prepared/{d_name}/train.csv', index=False)\n",
    "        valid_df.to_csv(f'prepared/{d_name}/valid.csv', index=False)\n",
    "        test_df.to_csv(f'prepared/{d_name}/test.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# prepare_magellan_row_pairs(magellan_dirs, magellan_rename_columns, magellan_drop_columns)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### WDC Datasets"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def prepare_wdc_row_pairs(dir: str):\n",
    "    used_columns = ['title_left', 'price_left', 'priceCurrency_left', 'label', 'title_right', 'price_right', 'priceCurrency_right']\n",
    "    train_df = pd.read_pickle(os.path.join(dir, 'train.pkl.gz'))[used_columns]\n",
    "    valid_df = pd.read_pickle(os.path.join(dir, 'valid.pkl.gz'))[used_columns]\n",
    "\n",
    "    merge_price_currency = lambda x, y: str(y) + str(x) if pd.notna(x) and pd.notna(y) else None\n",
    "    train_df['price_left'] = train_df.apply(lambda x: merge_price_currency(x['price_left'], x['priceCurrency_left']), axis=1)\n",
    "    train_df['price_right'] = train_df.apply(lambda x: merge_price_currency(x['price_right'], x['priceCurrency_right']), axis=1)\n",
    "    train_df.drop(columns=['priceCurrency_left', 'priceCurrency_right'], inplace=True)\n",
    "    train_df.columns = ['title_l', 'price_l', 'label', 'title_r', 'price_r']\n",
    "\n",
    "    valid_df['price_left'] = valid_df.apply(lambda x: str(x['price_left'])+ str(x['priceCurrency_left']), axis=1)\n",
    "    valid_df['price_right'] = valid_df.apply(lambda x: str(x['price_right'])+ str(x['priceCurrency_right']), axis=1)\n",
    "    valid_df.drop(columns=['priceCurrency_left', 'priceCurrency_right'], inplace=True)\n",
    "    valid_df.columns = ['title_l', 'price_l', 'label', 'title_r', 'price_r']\n",
    "\n",
    "    if not os.path.exists(f'prepared/wdc'):\n",
    "        os.makedirs(f'prepared/wdc')\n",
    "    train_df.to_csv(f'prepared/wdc/train.csv', index=False)\n",
    "    valid_df.to_csv(f'prepared/wdc/valid.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# prepare_wdc_row_pairs('raw/wdc')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Test Set"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Magellan Datasets\n",
    "The previous steps will generate a test set for each magellan dataset, while some of them will be overwritten by the following code."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# abt_buy\n",
    "used_columns = ['name_left', 'price_left', 'label', 'name_right', 'price_right']\n",
    "renamed_columns = ['name_l', 'price_l', 'label', 'name_r', 'price_r']\n",
    "abt_df = pd.read_pickle('raw/abt_buy/test.pkl.gz')[used_columns]\n",
    "abt_df.columns = renamed_columns\n",
    "abt_df.to_csv('prepared/abt/test.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# amgo\n",
    "test_magellan_used_columns = {\n",
    "    'abt': ['name_left', 'price_left', 'label', 'name_right', 'price_right'],\n",
    "    'amgo': ['title_left', 'price_left', 'label', 'title_right', 'price_right'],\n",
    "    'dbac': ['title_left', 'authors_left', 'venue_left', 'year_left', 'label', 'title_right', 'authors_right', 'venue_right', 'year_right'],\n",
    "    'dbgo': ['title_left', 'authors_left', 'venue_left', 'year_left', 'label', 'title_right', 'authors_right', 'venue_right', 'year_right'],\n",
    "    'waam': ['title_left', 'modelno_left', 'price_left', 'label', 'title_right', 'modelno_right', 'price_right']\n",
    "}\n",
    "\n",
    "test_magellan_rename_columns = {\n",
    "    'abt': ['name_l', 'price_l', 'label', 'name_r', 'price_r'],\n",
    "    'amgo': ['name_l', 'price_l', 'label', 'name_r', 'price_r'],\n",
    "    'dbac': ['title_l', 'authors_l', 'venue_l', 'year_l', 'label', 'title_r', 'authors_r', 'venue_r', 'year_r'],\n",
    "    'dbgo': ['title_l', 'authors_l', 'venue_l', 'year_l', 'label', 'title_r', 'authors_r', 'venue_r', 'year_r'],\n",
    "    'waam': ['name_l', 'modelno_l', 'price_l', 'label', 'name_r', 'modelno_r', 'price_r']\n",
    "}\n",
    "\n",
    "def prepare_test_magellan_row_pairs(dirs: dict, used_columns: dict, rename_columns: dict):\n",
    "    dirs = {key: dirs[key] for key in used_columns.keys() if key in dirs}\n",
    "    for d_name in dirs:\n",
    "        d_used_columns = used_columns[d_name]\n",
    "        d_rename_columns = rename_columns[d_name]\n",
    "        df = pd.read_pickle(f'{dirs[d_name]}/test.pkl.gz')[d_used_columns]\n",
    "        df.columns = d_rename_columns\n",
    "        df.to_csv(f'prepared/{d_name}/test.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# prepare_test_magellan_row_pairs(magellan_dirs, test_magellan_used_columns, test_magellan_rename_columns)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### WDC Datasets"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def prepare_test_wdc_row_pairs(dir: str):\n",
    "    used_columns = ['title_left', 'price_left', 'priceCurrency_left', 'label', 'title_right', 'price_right', 'priceCurrency_right']\n",
    "    test_df = pd.read_pickle(os.path.join(dir, 'test.pkl.gz'))[used_columns]\n",
    "\n",
    "    merge_price_currency = lambda x, y: str(y) + str(x) if pd.notna(x) and pd.notna(y) else None\n",
    "    test_df['price_left'] = test_df.apply(lambda x: merge_price_currency(x['price_left'], x['priceCurrency_left']), axis=1)\n",
    "    test_df['price_right'] = test_df.apply(lambda x: merge_price_currency(x['price_right'], x['priceCurrency_right']), axis=1)\n",
    "    test_df.drop(columns=['priceCurrency_left', 'priceCurrency_right', 'price_left', 'price_right'], inplace=True)\n",
    "    # test_df.columns = ['title_l', 'price_l', 'label', 'title_r', 'price_r']\n",
    "    test_df.columns = ['title_l', 'label', 'title_r'] # to align with the MatchGPT paper\n",
    "\n",
    "    test_df.to_csv(f'prepared/wdc/test.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# prepare_test_wdc_row_pairs('raw/wdc')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Attribute Pairs Preparation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataset_names = ['abt', 'amgo', 'beer', 'dbac', 'dbgo', 'foza', 'itam', 'waam', 'wdc']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def nan_check(value):\n",
    "    null_strings = [None, 'nan', 'NaN', 'NAN', 'null', 'NULL', 'Null', 'None', 'none', 'NONE', '', '-', '--', '---']\n",
    "    if pd.isna(value) or pd.isnull(value) or value in null_strings:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def numerical_check(value):\n",
    "    if isinstance(value, int) or isinstance(value, float):\n",
    "        return 1\n",
    "\n",
    "def string_identical_check(left_value, right_value, row_label):\n",
    "    if left_value == right_value or left_value in right_value or right_value in left_value:\n",
    "        return 1\n",
    "    else:\n",
    "        if row_label == 1:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "def numerical_identical_check(left_value, right_value, row_label):\n",
    "    if left_value == right_value:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def identical_check(left_value, right_value, row_label):\n",
    "    if nan_check(left_value) and not nan_check(right_value):\n",
    "        return 0\n",
    "    elif not nan_check(left_value) and nan_check(right_value):\n",
    "        return 0\n",
    "    elif nan_check(left_value) and nan_check(right_value):\n",
    "        return 1\n",
    "    elif numerical_check(left_value) and numerical_check(right_value):\n",
    "        return numerical_identical_check(left_value, right_value, row_label)\n",
    "    else:\n",
    "        left_value = str(left_value).lower()\n",
    "        right_value = str(right_value).lower()\n",
    "        return string_identical_check(left_value, right_value, row_label)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def row2attribute_pairs(row):\n",
    "    attr_pairs = []\n",
    "    all_columns = row.index\n",
    "    left_columns = [col for col in all_columns if col.endswith('_l')]\n",
    "    right_columns = [col for col in all_columns if col.endswith('_r')]\n",
    "    row_label = row['label']\n",
    "    for i in range(len(left_columns)):\n",
    "        left_value = row[left_columns[i]]\n",
    "        right_value = row[right_columns[i]]\n",
    "        attr_pair = [left_value, right_value, identical_check(left_value, right_value, row_label), left_columns[i][:-2]]\n",
    "        attr_pairs.append(attr_pair)\n",
    "    return attr_pairs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def prepare_all_attribute_pairs(names: list):\n",
    "    for name in names:\n",
    "        train_row_pairs = pd.read_csv(f'prepared/{name}/train.csv')\n",
    "        valid_row_pairs = pd.read_csv(f'prepared/{name}/valid.csv')\n",
    "        test_row_pairs = pd.read_csv(f'prepared/{name}/test.csv')\n",
    "        train_attr_pairs = []\n",
    "        valid_attr_pairs = []\n",
    "        test_attr_pairs = []\n",
    "\n",
    "        train_row_pairs.apply(lambda row: train_attr_pairs.extend(row2attribute_pairs(row)), axis=1)\n",
    "        valid_row_pairs.apply(lambda row: valid_attr_pairs.extend(row2attribute_pairs(row)), axis=1)\n",
    "        test_row_pairs.apply(lambda row: test_attr_pairs.extend(row2attribute_pairs(row)), axis=1)\n",
    "\n",
    "        train_attr_pairs_df = pd.DataFrame(train_attr_pairs, columns=['left_value', 'right_value', 'label', 'attribute'])\n",
    "        val_attr_pairs_df = pd.DataFrame(valid_attr_pairs, columns=['left_value', 'right_value', 'label', 'attribute'])\n",
    "        test_attr_pairs_df = pd.DataFrame(test_attr_pairs, columns=['left_value', 'right_value', 'label', 'attribute'])\n",
    "        train_attr_pairs_df.drop_duplicates(inplace=True)\n",
    "        val_attr_pairs_df.drop_duplicates(inplace=True)\n",
    "        test_attr_pairs_df.drop_duplicates(inplace=True)\n",
    "\n",
    "        train_attr_pairs_df.to_csv(f'prepared/{name}/attr_train.csv', index=False)\n",
    "        val_attr_pairs_df.to_csv(f'prepared/{name}/attr_valid.csv', index=False)\n",
    "        test_attr_pairs_df.to_csv(f'prepared/{name}/attr_test.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# prepare_all_attribute_pairs(dataset_names)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# AutoML Predictions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def prepare_automl_predictions():\n",
    "    dataset_names = ['abt', 'amgo', 'beer', 'dbac', 'dbgo', 'foza', 'itam', 'waam', 'wdc']\n",
    "    for name in dataset_names:\n",
    "        train_df = pd.read_csv(f'prepared/{name}/train.csv')\n",
    "        valid_df = pd.read_csv(f'prepared/{name}/valid.csv')\n",
    "\n",
    "        predictor = TabularPredictor(label='label').fit(train_data=train_df, tuning_data=valid_df, verbosity=-1)\n",
    "        train_preds = predictor.predict(train_df)\n",
    "        train_preds_proba = predictor.predict_proba(train_df)\n",
    "        valid_preds = predictor.predict(valid_df)\n",
    "        valid_preds_proba = predictor.predict_proba(valid_df)\n",
    "        train_preds_df = pd.DataFrame({'prediction': train_preds, 'proba_0': train_preds_proba[0], 'proba_1': train_preds_proba[1]})\n",
    "        valid_preds_df = pd.DataFrame({'prediction': valid_preds, 'proba_0': valid_preds_proba[0], 'proba_1': valid_preds_proba[1]})\n",
    "\n",
    "        if not os.path.exists(f'automl/{name}'):\n",
    "            os.makedirs(f'automl/{name}')\n",
    "        train_preds_df.to_csv(f'automl/{name}/train_preds.csv', index=False)\n",
    "        valid_preds_df.to_csv(f'automl/{name}/valid_preds.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "# prepare_automl_predictions()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
